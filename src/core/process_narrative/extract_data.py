"""This script extracts data from the PDF file using OpenAI."""

import json
import asyncio
# from concurrent.futures import ThreadPoolExecutor
import jsonschema
from jsonschema import validate
from langchain.prompts import PromptTemplate
from langchain.callbacks import get_openai_callback  # pylint: disable=no-name-in-module
from langchain.chains import LLMChain  # pylint: disable=no-name-in-module
from py_unified_cloud_adapter.utils.errors import CloudAdapterException
from dotenv import load_dotenv
from src.utils.log import logger
from src.utils.log import prompt_logger
from src.process_narrative.constants import PID_PROMPT
from src.process_narrative.constants import SCHEMA
from src.process_narrative.postprocessing.postprocessing import (
    process_narrative_post_processing,
)
from src.utils.llm_models.get_llm import get_llm_model
from src.process_narrative.postprocessing.post_process_data_and_save_to_s3 import (
    post_process_data_and_save_to_storage,
)
from src.utils.storage_utils import (
    upload_file_to_storage,
    fetch_file_via_adapter
)

load_dotenv()


def _chunks(data, n):
    """
    Generates chunks of size `n` from the given `data`.

    Parameters:
        data (iterable): The iterable from which chunks are generated.
        n (int): The size of each chunk.

    Yields:
        iterable: An iterable containing `n` elements from the `data` iterable.
    """
    for i in range(0, len(data), n):
        yield data[i : i + n]


def _get_prompt_template_name_and_response(
    get_prompt_template_name_and_response_args,
):
    """
    Retrieves the OpenAI API key from AWS Secrets Manager and
    uses it to generate a response to a prompt.

    Args:
        prompt_text (str): The text of the prompt.
        llm (LLM): The language model used to generate the response.
        assets_list_wo_narrative (list): A list of assets without narrative.
        asset (dict): The asset being processed.
        index (int): The index of the asset.
        pid_source_destination_connection_data (dict):
        The source-destination connection data for the process.

    Returns:
        tuple: A tuple containing the prompt template name,
        the OpenAI callback response, and the response generated by the language model.

    Raises:
        ClientError: If there is an error retrieving the secret value from AWS Secrets Manager.
    """
    (
        prompt_text,
        llm,
        assets_list_wo_narrative,
        asset,
        index,
        pid_source_destination_connection_data,
    ) = get_prompt_template_name_and_response_args
    source_destination_connection_json = pid_source_destination_connection_data
    prompt_template_name = PromptTemplate(
        input_variables=[
            "asset_json_string",
            "assets_json_string",
            "source_destination_connection_json",
            "narrative_id",
        ],
        template=prompt_text,
    )
    name_chain = LLMChain(llm=llm, prompt=prompt_template_name)
    with get_openai_callback() as cb_response:
        response = name_chain(
            {
                "asset_json_string": json.dumps(assets_list_wo_narrative, indent=2),
                "assets_json_string": asset,
                "source_destination_connection_json": source_destination_connection_json,
                "narrative_id": index + 1,
            }
        )
    return prompt_template_name, cb_response, response


async def _extract_asset_data_by_prompt(
    extract_asset_data_by_prompt_args,
):
    """
    Extracts asset data by prompt.

    Args:
        prompt_text (str): The prompt text.
        llm (LLM): The language model.
        assets_list_wo_narrative (list): The list of assets without narrative.
        index (int): The index.
        asset (dict): The asset.
        bucket_name (str): The name of the bucket.
        extracted_json_file_key (str): The key of the extracted JSON file.
        pid_source_destination_connection_data (dict): The data for source-destination connections.

    Returns:
        dict: The extracted asset data.
    """
    (
        prompt_text,
        llm,
        assets_list_wo_narrative,
        index,
        asset,
        bucket_name,
        extracted_json_file_key,
        pid_source_destination_connection_data,
    ) = extract_asset_data_by_prompt_args
    logger.info("INIT: Extract_Asset_Data_By_Prompt function initialized")
    _, cb_response, response = _get_prompt_template_name_and_response(
        (
            prompt_text,
            llm,
            assets_list_wo_narrative,
            asset,
            index,
            pid_source_destination_connection_data,
        )
    )
    prompt_logger.info(
        f"The number of tokens that our prompt is using is... \n {cb_response}"
    )
    result_string = response["text"].replace("```json", "").replace("```", "")
    result_object = json.loads(result_string)
    try:
        existing_data = await fetch_file_via_adapter(bucket_name, extracted_json_file_key)
    except CloudAdapterException:
        existing_data = []
    merged_data = existing_data + result_object
    await upload_file_to_storage(
        bucket_name,
        extracted_json_file_key,
        json.dumps(merged_data),
    )
    logger.info("DONE: Extract_Asset_Data_By_Prompt function completed")
    return result_object


def update_asset_id(data):
    """
    Update the "asset_id" field in each asset dictionary in the given data.

    Parameters:
        data (list): A list of dictionaries representing assets.

    Returns:
        None
    """
    for asset in data:
        if "asset_id" in asset:
            asset_id = asset["asset_id"]
            asset["asset_id"] = f"asset_{asset_id}"


async def modify_asset_id(bucket_name, connections_json_file_key):
    """
    Modifies the asset IDs in the given S3 bucket and key.

    Args:
        bucket_name (str): The name of the S3 bucket.
        connections_json_file_key (str): The key of the JSON file containing the asset IDs.

    Returns:
        None
    """
    try:
        data = await fetch_file_via_adapter(bucket_name, connections_json_file_key)
    except CloudAdapterException:
        data = []
    new_data = []
    for item in data:
        if "asset_id" in item:
            if isinstance(item["asset_id"], int):
                item["asset_id"] = f"asset_{item['asset_id']}"
                new_data.append(item)
    await upload_file_to_storage(
        bucket_name,
        connections_json_file_key,
        json.dumps(new_data),
    )
    logger.info(f"Modified data is uploaded back to S3 at {connections_json_file_key}")


async def _get_connection_data(
    bucket_name, extracted_json_file_key, connections_json_file_key
):
    """
    Retrieves connection data from an S3 bucket and modifies it.

    Args:
        bucket_name (str): The name of the S3 bucket.
        extracted_json_file_key (str): The key of the JSON file containing the existing data.
        connections_json_file_key (str): The key of the JSON file to store the modified connections.

    Returns:
        None
    """
    logger.info("INIT: Get Connection Data function initialized")
    try:
        existing_data = await fetch_file_via_adapter(
            bucket_name, extracted_json_file_key
        )
    except CloudAdapterException:
        existing_data = []
    modified_connections = []
    await upload_file_to_storage(
        bucket_name,
        connections_json_file_key,
        json.dumps(modified_connections),
    )
    for asset in existing_data:
        asset_id = asset.get("asset_id")
        connections = asset.get("connections", [])
        for connection in connections:
            connection["asset_id"] = asset_id
            modified_connections.append(connection)
        del asset["connections"]
    await upload_file_to_storage(
        bucket_name,
        connections_json_file_key,
        json.dumps(modified_connections),
    )


async def _generate_connection_id(bucket_name, extracted_json_file_key):
    """
    Generates connection IDs for assets in a bucket/container.
    """
    logger.info("INIT: Generate Connection IDs function initialized")
    try:
        extracted_data = await fetch_file_via_adapter(
            bucket_name,
            extracted_json_file_key
        )
    except CloudAdapterException:
        extracted_data = []

    connection_counter = 1
    for asset in extracted_data:
        asset_title = f"{asset['asset_name']} {asset['asset_tag']}"
        asset["asset_title"] = asset_title
        asset = {k: asset[k] for k in sorted(asset, key=str)}
        connections = asset.get("connections", [])
        for connection in connections:
            connection["connection_id"] = f"connection_{connection_counter}"
            connection["id"] = f"connection_{connection_counter}"
            connection_counter += 1
            connection = {k: connection[k] for k in sorted(connection, key=str)}
    await upload_file_to_storage(
        bucket_name,
        extracted_json_file_key,
        json.dumps(extracted_data),
    )
    logger.info("DONE: Generate Connection IDs function completed")


async def _process_asset(
    process_asset_args,
):
    """
    Process an asset by extracting data using a prompt.

    Args:
        index (int): The index of the asset.
        asset (dict): The asset to process.
        number_of_paragraphs (int): The total number of paragraphs.
        prompt_text (str): The prompt text.
        llm (LLM): The language model.
        assets_list_wo_narrative (list): The list of assets without narrative.
        results (list): The list to store the result objects.
        bucket_name (str): The name of the S3 bucket.
        extracted_json_file_key (str): The key of the extracted JSON file in the S3 bucket.
        pid_source_destination_connection_data (dict):
        The dictionary containing source-destination connection data.

    Returns:
        dict: The result object.
    """
    (
        index,
        asset,
        number_of_paragraphs,
        prompt_text,
        llm,
        assets_list_wo_narrative,
        results,
        bucket_name,
        extracted_json_file_key,
        pid_source_destination_connection_data,
    ) = process_asset_args
    logger.info(f"INIT:Processing paragraph ({index + 1}/{number_of_paragraphs})")
    result_object = await _extract_asset_data_by_prompt(
        (
            prompt_text,
            llm,
            assets_list_wo_narrative,
            index,
            asset,
            bucket_name,
            extracted_json_file_key,
            pid_source_destination_connection_data,
        )
    )
    results.append(result_object)
    logger.info(f"DONE: Processing paragraph ({index + 1}/{number_of_paragraphs})")
    return result_object


async def schema_validation(bucket_name, extracted_json_file_key):
    """
    Validates the JSON data in the specified S3 bucket and key against a predefined schema.

    Args:
        bucket_name (str): The name of the S3 bucket.
        extracted_json_file_key (str): The key of the JSON file in the S3 bucket.

    Returns:
        None

    Raises:
        jsonschema.exceptions.ValidationError: If the JSON data does not conform to the schema.

    """
    logger.info("INIT: Schema Validation function initialized")
    connection_data = await fetch_file_via_adapter(
        bucket_name,
        extracted_json_file_key
    )
    data_schema = json.loads(SCHEMA)
    try:
        validate(instance=connection_data, schema=data_schema)
        logger.info("JSON data is valid")
    except jsonschema.exceptions.ValidationError as e:
        logger.info("JSON data is not valid")
        print(e)


async def extract_data(
    process_narrative_args,
):
    """
    Extracts data from the given narrative and asset texts,
    and saves the extracted data to an S3 bucket.

    Args:
        narrative_text (str): The text of the narrative.
        asset_text (str): The text of the asset.
        bucket_name (str): The name of the S3 bucket.
        pnid_connections_json_path (str): The path to the PNID connections JSON file.
        extracted_json_file_key (str): The key of the extracted JSON file in the S3 bucket.
        connections_json_file_key (str): The key of the connections JSON file in the S3 bucket.
        model_name (str): The name of the model to be used.
        pid_source_destination_connection_data (str): The PID source-destination connection data.
        post_process_connection_path (str): The path to the post-process connection file.
        expected_asset_csv_s3 (str): The expected asset CSV file in cloud_storage
        narrative_csv_path (str): The path to the narrative CSV file.
        input_bounding_box_config_path (str): The path to the input bounding box configuration file.

    Returns:
        None

    Raises:
        None
    """
    (
        narrative_text,
        asset_text,
        bucket_name,
        pnid_connections_json_path,
        extracted_json_file_key,
        connections_json_file_key,
        model_name,
        pid_source_destination_connection_data,
        post_process_connection_path,
        input_bounding_box_config_path,
    ) = process_narrative_args
    logger.info("INIT: Extract_Asset_Data function initialized")
    asset_csv = asset_text
    asset_text = narrative_text
    asset_list_df = asset_csv
    assets_list_wo_narrative = asset_list_df.to_dict(orient="records")
    asset_list = asset_text["text"]
    prompt_text = ""
    number_of_paragraphs = len(asset_list)
    prompt_text = PID_PROMPT
    llm = get_llm_model(model_name=model_name)
    results = []
    empty_string = []
    await upload_file_to_storage(
        bucket_name,
        extracted_json_file_key,
        json.dumps(empty_string),
    )

    for asset_chunk in _chunks(list(enumerate(asset_list)), 4):
        tasks = []
        for item in asset_chunk:
            # Create the tuple of arguments
            args_tuple = (
                item[0],
                item[1],
                number_of_paragraphs,
                prompt_text,
                llm,
                assets_list_wo_narrative,
                results,
                bucket_name,
                extracted_json_file_key,
                pid_source_destination_connection_data,
            )
            tasks.append(_process_asset(args_tuple))

        chunk_results = await asyncio.gather(*tasks)
        results.extend(chunk_results)
    await _generate_connection_id(
        bucket_name=bucket_name, extracted_json_file_key=extracted_json_file_key
    )
    await post_process_data_and_save_to_storage(
        bucket_name=bucket_name, extracted_json_file_key=extracted_json_file_key
    )
    await _get_connection_data(
        bucket_name=bucket_name,
        extracted_json_file_key=extracted_json_file_key,
        connections_json_file_key=connections_json_file_key,
    )
    # modify_asset_id(bucket_name=bucket_name, connections_json_file_key=connections_json_file_key)
    await process_narrative_post_processing(
        (
            bucket_name,
            connections_json_file_key,
            pnid_connections_json_path,
            post_process_connection_path,
            input_bounding_box_config_path,
        )
    )
    # schema_validation(
    #     bucket_name=bucket_name, extracted_json_file_key=extracted_json_file_key
    # )
    logger.info("DONE: Extract_Asset_Data function completed")


if __name__ == "__main__":
    asyncio.run(extract_data(process_narrative_args=()))
